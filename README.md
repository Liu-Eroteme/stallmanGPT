
# StallmanGPT

StallmanGPT is a simple command-line tool that generates Linux commands based on your plain text request. It uses OpenAI's GPT-3.5-turbo to generate the commands and iteratively improve them based on a confidence rating.

## Installation

To install StallmanGPT from PyPI, run the following command:

```bash
pip install stallmanGPT
```

After installation, make sure to set the OPENAI_API_KEY environment variable to your OpenAI API key. StallmanGPT will not work without it.

Usage
To use StallmanGPT, simply run the command followed by your plain text request:

```bash
stallmanGPT "search my pc and show me all PDF files from April 2020"
```

The tool will generate a Linux command, provide an explanation of the command, and prompt you for confirmation to execute it.

## Limitations and Known Issues

- The quality of the generated commands depends on the performance of OpenAI's GPT-3.5-turbo.
- The tool relies on an OpenAI API key and may be subject to rate limits and usage costs.
- The confidence rating may not always accurately reflect the effectiveness of the generated command.

## Contributing

We welcome contributions to the StallmanGPT project! Here are some basic guidelines to follow:

1. Fork the repository on GitHub.
2. Create a new branch for your changes.
3. Make your changes and commit them to your branch.
4. Submit a pull request to the main repository, and provide a clear description of the changes you made.

We appreciate your help in making StallmanGPT even better!

Generated by GPT4

ERROR:

I have installed the script locally using "sudo pip install ." 
Running the command "stallmangpt "find all .md files and list them" produces the following error:

Traceback (most recent call last):
  File "/home/liu/.local/bin/stallmangpt", line 8, in <module>
    sys.exit(main())
  File "/home/liu/.local/lib/python3.10/site-packages/stallmangpt/__init__.py", line 46, in main
    command = prompt_openai(prompt)
  File "/home/liu/.local/lib/python3.10/site-packages/stallmangpt/__init__.py", line 17, in prompt_openai
    response = openai.Completion.create(engine="gpt-3.5-turbo", prompt=prompt, max_tokens=150, n=1, stop=None, temperature=0.5)
  File "/home/liu/.local/lib/python3.10/site-packages/openai/api_resources/completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/home/liu/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/home/liu/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/home/liu/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 620, in _interpret_response
    self._interpret_response_line(
  File "/home/liu/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 683, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?

Please analyze this error message, explain the error, analyze your code and propose a solution.
